# fap_search_engine.py
import pandas as pd
import numpy as np
import uuid
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, MatchValue, Range
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
def content_hash(content: str) -> str:
    import hashlib
    return hashlib.sha256(content.encode('utf-8')).hexdigest()
from FAP.llm_helper import LLMHelper
from dotenv import load_dotenv

class FapSearchEngine:
    def __init__(self, csv_paths: dict, qdrant_url: str = None, qdrant_api_key: str = None, collection_name: str = None, enable_llm: bool = False):
        """
        Kh·ªüi t·∫°o client, load CSV, kh·ªüi t·∫°o embedder + config + LLM
        """
        load_dotenv()
        # Qdrant config
        self.qdrant_url = qdrant_url or os.environ.get("QDRANT_URL")
        self.qdrant_api_key = qdrant_api_key or os.environ.get("QDRANT_API_KEY")
        self.collection_name = collection_name or os.environ.get("QDRANT_COLLECTION", "Fap_data_testing")
        print(self.qdrant_url, self.qdrant_api_key, self.collection_name)
        # Kh·ªüi t·∫°o Qdrant client
        self.client = QdrantClient(
            url=self.qdrant_url,
            api_key=self.qdrant_api_key,
            prefer_grpc=False
        )
        # T·ª± ƒë·ªông t·∫°o collection n·∫øu ch∆∞a c√≥
        try:
            self.client.get_collection(self.collection_name)
            print(f"‚úÖ Collection {self.collection_name} already exists.")
        except Exception:
            print(f"‚ö†Ô∏è Collection {self.collection_name} not found. Creating...")
            self.client.recreate_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
            )
            print(f"‚úÖ Created collection {self.collection_name}.")
        
        # CSV paths
        self.csv_paths = csv_paths
        
        # DataFrames storage
        self.dataframes = {}
        
        # Kh·ªüi t·∫°o BGE-M3 embedder
        self.embedder = SentenceTransformer("BAAI/bge-m3")
        self.prefix = "Represent this sentence for searching relevant passages: "
        
        # Embedding caches cho detection
        self.subject_embeddings = {}
        self.type_embeddings = {}
        self.term_embeddings = {}
        
        # LLM Helper
        self.enable_llm = enable_llm
        self.llm_helper = LLMHelper() if enable_llm else None
        if enable_llm and self.llm_helper.is_available():
            print(f"‚úÖ LLM enabled with {self.llm_helper.model}")
        else:
            print("‚ö†Ô∏è LLM disabled or not available")
        
        print(f"‚úÖ Fap Search Engine initialized with collection: {self.collection_name}")
    
    def _normalize_date_format(self, date_str: str) -> str:
        """
        Chuy·ªÉn ƒë·ªïi format ng√†y t·ª´ "Monday 09/09/2024" th√†nh "09/09/2024"
        """
        if not date_str or date_str == "Kh√¥ng r√µ":
            return "Kh√¥ng r√µ"
        
        # T√¨m pattern ng√†y th√°ng trong chu·ªói
        import re
        date_pattern = r'(\d{1,2})/(\d{1,2})/(\d{4})'
        match = re.search(date_pattern, date_str)
        
        if match:
            day, month, year = match.groups()
            return f"{day.zfill(2)}/{month.zfill(2)}/{year}"
        
        return date_str
    
    def load_all_dataframes(self):
        """
        ƒê·ªçc to√†n b·ªô d·ªØ li·ªáu CSV v√†o c√°c DataFrame:
        - student_profile
        - attendance_reports
        - grade_details
        - course_summaries
        """
        expected_keys = ['student_profile', 'attendance_reports', 'grade_details', 'course_summaries']
        
        for key in expected_keys:
            if key in self.csv_paths:
                path = self.csv_paths[key]
                if os.path.exists(path):
                    self.dataframes[key] = pd.read_csv(path)
                    print(f"‚úÖ Loaded {key}: {len(self.dataframes[key])} rows")
                else:
                    print(f"‚ùå File not found: {path}")
            else:
                print(f"‚ö†Ô∏è  Missing key in csv_paths: {key}")
        
        print(f"üìä Total DataFrames loaded: {len(self.dataframes)}")
    
    def chunk_student_profile(self, df, user_full_name=None):
        """
        Build payloads cho b·∫£ng student_profile
        ‚Üí output: list[dict]
        """
        def safe(x):
            return str(x).strip() if pd.notnull(x) else "Kh√¥ng r√µ"
        payloads = []
        
        for _, row in df.iterrows():
            full_name = user_full_name or safe(row.get('full_name', ''))
            roll_number = safe(row.get('roll_number', ''))
            noi_dung = (
                f"Th√¥ng tin sinh vi√™n: {full_name} | M√£ SV: {roll_number}\n"
                f"Ng√†y sinh: {safe(row['date_of_birth'])} | Gi·ªõi t√≠nh: {safe(row['gender'])}\n"
                f"Chuy√™n ng√†nh: {safe(row['major'])} | L·ªõp: {safe(row['main_class'])} | Tr·∫°ng th√°i: {safe(row['current_status'])}\n"
                f"ƒê√†o t·∫°o: {'Ch√≠nh quy' if bool(row['is_full_time_student']) else 'Kh√¥ng ch√≠nh quy'} | H·ªçc b·ªïng: {'C√≥' if bool(row['is_scholarship_student']) else 'Kh√¥ng'}\n"
                f"ƒê·ªãa ch·ªâ: {safe(row['home_address'])} | Email: {safe(row['email_address'])} | SƒêT: {safe(row['phone_number'])}"
            )
            payload = {
                "user_full_name": full_name,
                "user_id": roll_number,
                "ho_ten": full_name,
                "ngay_sinh": safe(row["date_of_birth"]),
                "gioi_tinh": safe(row["gender"]),
                "so_cmnd": safe(row["id_card_number"]),
                "ngay_cap_cmnd": safe(row["id_date_of_issue"]),
                "noi_cap_cmnd": safe(row["id_place_of_issue"]),
                "dia_chi": safe(row["home_address"]),
                "sdt": safe(row["phone_number"]),
                "email": safe(row["email_address"]),
                "ma_sinh_vien": safe(row["roll_number"]),
                "ma_cu": safe(row["old_roll_number"]),
                "ma_thanh_vien": safe(row["member_code"]),
                "ngay_nhap_hoc": safe(row["enrollment_date"]),
                "chuyen_nganh": safe(row["major"]),
                "lop_chinh": safe(row["main_class"]),
                "trang_thai_hoc_tap": safe(row["current_status"]),
                "sinh_vien_chinh_quy": bool(row["is_full_time_student"]),
                "sinh_vien_hoc_bong": bool(row["is_scholarship_student"]),
                "loai_dao_tao": safe(row["training_type"]),
                "hoc_ky_bat_dau": safe(row["start_term"]),
                "loai": "th√¥ng tin sinh vi√™n",
                "noi_dung": noi_dung,
                "content_hash": content_hash(noi_dung)
            }
            payloads.append(payload)
        return payloads
    def chunk_attendance_reports(self, df, user_full_name=None):
        """
        Build payloads cho b·∫£ng attendance_reports
        """
        def safe(x):
            return str(x).strip() if pd.notnull(x) else "Kh√¥ng r√µ"
        
        payloads = []
        for _, row in df.iterrows():
            full_name = user_full_name or safe(row.get('full_name', ''))
            student_id = safe(row.get('student_id', ''))
            noi_dung = (
                f"LO·∫†I: ƒêi·ªÉm danh\n"
                f"Sinh vi√™n: {full_name} ({student_id})\n"
                f"M√¥n h·ªçc: {safe(row['course_code'])} - {safe(row['course_name'])}\n"
                f"H·ªçc k·ª≥: {safe(row['term'])} | Bu·ªïi s·ªë: {safe(row['no'])} - Ng√†y: {safe(row['date'])} - Ca: {safe(row['slot'])} - Ph√≤ng: {safe(row['room'])}\n"
                f"Gi·∫£ng vi√™n: {safe(row['lecturer'])} | Nh√≥m: {safe(row['group'])}\n"
                f"Tr·∫°ng th√°i: {safe(row['status'])} | Ghi ch√∫: {safe(row['comment'])}"
            )
            payload = {
                "user_full_name": full_name,
                "user_id": student_id,
                "ma_sinh_vien": safe(row["student_id"]),
                "hoc_ky": safe(row["term"]),
                "ten_mon_hoc": safe(row["course_name"]),
                "ma_mon_hoc": safe(row["course_code"]),
                "buoi_so": int(row["no"]) if pd.notnull(row["no"]) else -1,
                "ngay": self._normalize_date_format(safe(row["date"])),
                "ca_hoc": safe(row["slot"]),
                "phong": safe(row["room"]),
                "giang_vien": safe(row["lecturer"]),
                "nhom_lop": safe(row["group"]),
                "trang_thai": safe(row["status"]),
                "ghi_chu": safe(row["comment"]),
                "loai": "ƒëi·ªÉm danh",
                "noi_dung": noi_dung,
                "content_hash": content_hash(noi_dung)
            }
            payloads.append(payload)
        print(f"üìù Generated {len(payloads)} attendance report payloads")
        return payloads
    
    def chunk_grade_details(self, df, user_full_name=None):
        """
        Build payloads cho b·∫£ng grade_details
        """
        def safe(x):
            return str(x).strip() if pd.notnull(x) else "Kh√¥ng r√µ"
        payloads = []
        
        for _, row in df.iterrows():
            full_name = user_full_name or safe(row.get('full_name', ''))
            student_id = safe(row.get('student_id', ''))
            noi_dung = (
                f"LO·∫†I: Chi ti·∫øt ƒëi·ªÉm\n"
                f"Sinh vi√™n: {full_name} ({student_id})\n"
                f"M√¥n h·ªçc: {safe(row['course_code'])} - {safe(row['course_name'])}\n"
                f"H·ªçc k·ª≥: {safe(row['term'])}\n"
                f"M·ª•c: {safe(row['item'])} | Lo·∫°i: {safe(row['category'])}\n"
                f"Tr·ªçng s·ªë: {safe(row['weight'])} | ƒêi·ªÉm ƒë·∫°t: {safe(row['value'])}"
            )
            payload = {
                "user_full_name": full_name,
                "user_id": student_id,
                "ma_sinh_vien": safe(row["student_id"]),
                "hoc_ky": safe(row["term"]),
                "ten_mon_hoc": safe(row["course_name"]),
                "ma_mon_hoc": safe(row["course_code"]),
                "phan_loai": safe(row["category"]),
                "muc_danh_gia": safe(row["item"]),
                "trong_so": safe(row["weight"]),
                "gia_tri_diem": row["value"] if pd.notnull(row["value"]) else -1.0,
                "loai": "chi ti·∫øt ƒëi·ªÉm",
                "noi_dung": noi_dung,
                "content_hash": content_hash(noi_dung)
            }
            payloads.append(payload)
        
        print(f"üìù Generated {len(payloads)} grade detail payloads")
        return payloads
    
    def chunk_course_summaries(self, df, user_full_name=None):
        """
        Build payloads cho b·∫£ng course_summaries
        """
        def safe(x):
            return str(x).strip() if pd.notnull(x) else "Kh√¥ng r√µ"
        payloads = []
        
        for _, row in df.iterrows():
            full_name = user_full_name or ""
            noi_dung = (
                f"LO·∫†I: t·ªïng k·∫øt m√¥n h·ªçc\n"
                f"Sinh vi√™n: {full_name}\n"
                f"M√¥n h·ªçc: {safe(row['course_code'])} - {safe(row['course_name'])}\n"
                f"H·ªçc k·ª≥: {safe(row['term'])}\n"
                f"ƒêi·ªÉm trung b√¨nh: {safe(row['avg_score'])}\n"
                f"Tr·∫°ng th√°i: {safe(row['status'])}\n"
                f"T√≥m t·∫Øt: {safe(row['summary'])}"
            )
            payload = {
                "user_full_name": full_name,
                "user_id": None,
                "hoc_ky": safe(row["term"]),
                "ten_mon_hoc": safe(row["course_name"]),
                "ma_mon_hoc": safe(row["course_code"]),
                "diem_trung_binh": float(row["avg_score"]) if pd.notnull(row["avg_score"]) else -1.0,
                "trang_thai": safe(row["status"]),
                "tom_tat": safe(row["summary"]),
                "loai": "t·ªïng k·∫øt m√¥n h·ªçc",
                "noi_dung": noi_dung,
                "content_hash": content_hash(noi_dung)
            }
            payloads.append(payload)
        
        print(f"üìù Generated {len(payloads)} course summary payloads")
        return payloads
    
    def generate_content_embedding(self, payloads: list[dict]):
        """
        Nh√∫ng field 'noi_dung' c·ªßa payloads b·∫±ng BGE-M3
        """
        # L·∫•y c√°c n·ªôi dung c·∫ßn embedding
        contents = []
        for payload in payloads:
            if "noi_dung" in payload:
                content = payload["noi_dung"]
                # Th√™m prefix cho BGE-M3
                if not content.startswith(self.prefix):
                    content = self.prefix + content
                contents.append(content)
            else:
                contents.append(self.prefix + "")
        
        print(f"üîÑ Embedding {len(contents)} contents with BGE-M3...")
        
        # T·∫°o embeddings
        embeddings = self.embedder.encode(
            contents, 
            batch_size=64, 
            normalize_embeddings=True,
            show_progress_bar=True
        )
        
        print(f"‚úÖ Generated {len(embeddings)} embeddings, shape: {embeddings.shape}")
        return embeddings
    
    def merge_point_structs(self, payloads, embeddings):
        """
        T·∫°o list PointStruct t·ª´ embedding + payloads
        """
        points = []
        
        for i, (payload, embedding) in enumerate(zip(payloads, embeddings)):
            point = PointStruct(
                id=str(uuid.uuid4()),
                vector=embedding.tolist(),
                payload=payload
            )
            points.append(point)
        
        print(f"üì¶ Created {len(points)} PointStruct objects")
        return points
    
    def get_existing_hashes(self):
        """
        L·∫•y t·∫•t c·∫£ content_hash ƒë√£ c√≥ trong collection Qdrant.
        """
        existing_hashes = set()
        try:
            scroll = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=None,
                limit=10000,  # T√πy ch·ªânh n·∫øu d·ªØ li·ªáu l·ªõn
                with_payload=["content_hash"]
            )
            for point in scroll[0]:
                h = point.payload.get("content_hash")
                if h:
                    existing_hashes.add(h)
        except Exception as e:
            print(f"‚ùå Error getting existing hashes: {e}")
        return existing_hashes

    def safe_upsert_to_qdrant(self, points: list, batch_size: int = 100, user_id: str = None):
        """
        Upsert theo batch an to√†n v·ªõi ki·ªÉm tra duplicate content_hash
        H·ªó tr·ª£ ki·ªÉm tra tr√πng l·∫∑p theo user n·∫øu user_id ƒë∆∞·ª£c cung c·∫•p
        """
        total_points = len(points)
        success_count = 0
        print(f"üì§ Upserting {total_points} points in batches of {batch_size} (with duplicate check)...")
        
        # L·∫•y existing hashes
        if user_id:
            existing_hashes = self.get_existing_hashes_for_user(user_id)
            print(f"üîí User-specific duplicate check for user: {user_id}")
        else:
            existing_hashes = self.get_existing_hashes()
            print("‚ö†Ô∏è  Global duplicate check (no user_id provided)")
        
        filtered_points = []
        for point in points:
            h = point.payload.get("content_hash")
            if h not in existing_hashes:
                filtered_points.append(point)
                existing_hashes.add(h)
            # N·∫øu mu·ªën update khi n·ªôi dung kh√°c, c√≥ th·ªÉ th√™m logic ·ªü ƒë√¢y
        
        print(f"‚û°Ô∏è  {len(filtered_points)}/{total_points} points to upsert (unique by content_hash)")
        
        for i in range(0, len(filtered_points), batch_size):
            batch = filtered_points[i:i + batch_size]
            retry_count = 0
            max_retries = 3
            while retry_count < max_retries:
                try:
                    self.client.upsert(
                        collection_name=self.collection_name,
                        points=batch
                    )
                    success_count += len(batch)
                    print(f"‚úÖ Batch {i//batch_size + 1}: {len(batch)} points uploaded")
                    break
                except Exception as e:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"‚ö†Ô∏è  Retry {retry_count}/3 for batch {i//batch_size + 1}: {e}")
                        time.sleep(2 ** retry_count)
                    else:
                        print(f"‚ùå Failed batch {i//batch_size + 1} after {max_retries} retries: {e}")
        
        print(f"üéØ Successfully uploaded {success_count}/{total_points} points (unique)")
        return success_count
    
    def create_payload_index(self):
        """
        T·∫°o c√°c index filter cho c√°c field:
        - user_id (b·∫£o m·∫≠t)
        - loai
        - hoc_ky
        - ma_mon_hoc
        """
        index_fields = [
            ("user_id", "keyword"),  # üîí B·∫£o m·∫≠t: index cho user_id
            ("loai", "keyword"),
            ("hoc_ky", "keyword"), 
            ("ma_mon_hoc", "keyword"),
            ("ngay", "keyword"),  # ‚è∞ Time range filtering
        ]
        
        for field_name, field_schema in index_fields:
            try:
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name=field_name,
                    field_schema=field_schema
                )
                print(f"‚úÖ Created index for field: {field_name}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Index creation failed for {field_name}: {e}")
    
    def create_subject_embeddings(self):
        """
        T·∫°o embedding cho t·ª´ng m√¥n h·ªçc t·ª´ course_code + course_name
        """
        if 'course_summaries' not in self.dataframes:
            print("‚ö†Ô∏è  No course_summaries data available")
            return
        
        df = self.dataframes['course_summaries']
        subjects_texts = []
        subjects_code = []

        
        for _, row in df.iterrows():
            course_code = row.get('course_code', '')
            course_name = row.get('course_name', '')
            subject_text = f"{course_code} - {course_name}".strip()
            if subject_text and subject_text not in subjects_texts:
                subjects_texts.append(subject_text)
            if course_code and course_code not in subjects_code:
                subjects_code.append(course_code)
        
        if subjects_texts:
            # T·∫°o embedding cho c√°c m√¥n h·ªçc
            embeddings = self.generate_content_embedding(subjects_texts)
            
            for subject_code, embedding in zip(subjects_code, embeddings):
                self.subject_embeddings[subject_code] = embedding
            
            print(f"üìö Created embeddings for {len(subjects_code)} subjects")
    
    def create_type_embeddings(self):
        """
        T·∫°o embedding cho t·ª´ng lo·∫°i d·ªØ li·ªáu: ƒëi·ªÉm danh, chi ti·∫øt ƒëi·ªÉm,...
        """
        type_descriptions = {
            "t·ªïng k·∫øt m√¥n h·ªçc": "T√≥m t·∫Øt v√† ƒë√°nh gi√° t·ªïng quan v·ªÅ m·ªôt m√¥n h·ªçc, bao g·ªìm ƒëi·ªÉm trung b√¨nh, tr·∫°ng th√°i v√† ƒëi·ªÉm s·ªë t·ªïng quan c√°c lo·∫°i ƒëi·ªÉm nh∆∞ lab, assignment, Participation, final exam.",
            "chi ti·∫øt ƒëi·ªÉm": "Th√¥ng tin chi ti·∫øt v·ªÅ ƒëi·ªÉm s·ªë c·ªßa sinh vi√™n trong c√°c m·ª•c ƒë√°nh gi√° kh√°c nhau, bao g·ªìm ph√¢n lo·∫°i, tr·ªçng s·ªë v√† gi√° tr·ªã ƒëi·ªÉm chi ti·∫øt, v√≠ d·ª• lab 1, lab 2, lab 3, progress test 1, 2, 3,...",
            "ƒëi·ªÉm danh": "Th√¥ng tin v·ªÅ vi·ªác ƒëi·ªÉm danh, ho·∫∑c th·ªùi gian bi·ªÉu, l·ªãch h·ªçc sinh vi√™n trong c√°c bu·ªïi h·ªçc, bao g·ªìm ng√†y, ca h·ªçc, ph√≤ng h·ªçc, gi·∫£ng vi√™n v√† tr·∫°ng th√°i ƒëi·ªÉm danh.",
            "th√¥ng tin sinh vi√™n": "Th√¥ng tin c√° nh√¢n c·ªßa sinh vi√™n, bao g·ªìm h·ªç t√™n, ng√†y sinh, gi·ªõi t√≠nh, ƒë·ªãa ch·ªâ, m√£ sinh vi√™n v√† c√°c th√¥ng tin li√™n quan ƒë·∫øn sinh vi√™n.",
        }
        
        # T·∫°o embedding cho c√°c lo·∫°i d·ªØ li·ªáu
        type_texts = list(type_descriptions.values())
        embeddings = self.generate_content_embedding(type_texts)
        
        for data_type, embedding in zip(type_descriptions.keys(), embeddings):
            self.type_embeddings[data_type] = embedding
        
        print(f"üè∑Ô∏è  Created embeddings for {len(type_descriptions)} data types")
    
    def create_term_embeddings(self):
        """
        T·∫°o embedding cho t·ª´ng h·ªçc k·ª≥ (Fall2023, Spring2024,...)
        """
        term_descriptions = {
        "Fall2023": "H·ªçc k·ª≥ m√πa thu nƒÉm 2023, di·ªÖn ra t·ª´ th√°ng 9 ƒë·∫øn th√°ng 12.",
        "Spring2024": "H·ªçc k·ª≥ m√πa xu√¢n nƒÉm 2024, b·∫Øt ƒë·∫ßu t·ª´ th√°ng 1 ƒë·∫øn th√°ng 4.",
        "Summer2024": "H·ªçc k·ª≥ h√® nƒÉm 2024, di·ªÖn ra t·ª´ th√°ng 5 ƒë·∫øn th√°ng 8.",
        "Fall2024": "H·ªçc k·ª≥ m√πa thu nƒÉm 2024, t·ª´ th√°ng 9 ƒë·∫øn th√°ng 12.",
        "Spring2025": "H·ªçc k·ª≥ m√πa xu√¢n nƒÉm 2025, t·ª´ th√°ng 1 ƒë·∫øn th√°ng 4."
        }
        type_texts = list(term_descriptions.values())
        embeddings = self.generate_content_embedding(type_texts)
        
        for data_type, embedding in zip(term_descriptions.keys(), embeddings):
            self.term_embeddings[data_type] = embedding
        
        print(f"üè∑Ô∏è  Created embeddings for {len(term_descriptions)} data types")
    
    def detect_subject_from_query(self, query, threshold=0.3, return_score=False):
        if not self.subject_embeddings:
            return (None, 0) if return_score else None
        query_embedding = self.generate_content_embedding([query])[0]
        best_match = None
        best_score = 0
        for subject, subject_embedding in self.subject_embeddings.items():
            similarity = cosine_similarity([query_embedding], [subject_embedding])[0][0]
            if similarity > threshold and similarity > best_score:
                best_score = similarity
                best_match = subject
        if return_score:
            return (best_match, best_score)
        return best_match
    
    def detect_type_from_query(self, query, threshold=0.3, return_score=False):
        if not self.type_embeddings:
            return (None, 0) if return_score else None
        query_embedding = self.generate_content_embedding([query])[0]
        best_match = None
        best_score = 0
        for data_type, type_embedding in self.type_embeddings.items():
            similarity = cosine_similarity([query_embedding], [type_embedding])[0][0]
            if similarity > threshold and similarity > best_score:
                best_score = similarity
                best_match = data_type
        if return_score:
            return (best_match, best_score)
        return best_match
    
    def detect_term_from_query(self, query, threshold=0.3, return_score=False):
        if not self.term_embeddings:
            return (None, 0) if return_score else None
        query_embedding = self.generate_content_embedding([query])[0]
        best_match = None
        best_score = 0
        for term, term_embedding in self.term_embeddings.items():
            similarity = cosine_similarity([query_embedding], [term_embedding])[0][0]
            if similarity > threshold and similarity > best_score:
                best_score = similarity
                best_match = term
        if return_score:
            return (best_match, best_score)
        return best_match
    
    def search_qdrant(self, query: str, user_id: str = None, limit: int = 5, threshold: float = 0.3, chat_history: list = None):
        """
        H√†m search ch√≠nh, detect ‚Üí filter ‚Üí truy v·∫•n Qdrant ‚Üí LLM enhance
        Tr·∫£ v·ªÅ list k·∫øt qu·∫£ (rank, score, type, subject, content)
        """
        print(f"üîç Searching for: '{query}' (User: {user_id})")
        
        # LLM Extract Intent (n·∫øu enabled)
        llm_intent = {}
        if self.enable_llm and self.llm_helper:
            llm_intent = self.llm_helper.extract_query_intent(query, chat_history=chat_history)
            print(f"ü§ñ LLM Intent: {llm_intent}")
        
        # Detect c√°c th√¥ng tin t·ª´ query (backup khi LLM fail)
        detected_subject, subject_score = self.detect_subject_from_query(query, threshold, return_score=True)
        detected_type, type_score = self.detect_type_from_query(query, threshold, return_score=True)
        
        # ∆Øu ti√™n LLM intent n·∫øu c√≥ (s·ª≠ d·ª•ng t√™n tr∆∞·ªùng m·ªõi)
        if llm_intent.get('ma_mon_hoc'):
            detected_subject = llm_intent['ma_mon_hoc']
        if llm_intent.get('loai'):
            detected_type = llm_intent['loai']
        
        # Time range filtering t·ª´ LLM intent
        time_range_filter = None
        if llm_intent.get('time_range'):
            time_range = llm_intent['time_range']
            if isinstance(time_range, dict) and 'start_date' in time_range and 'end_date' in time_range:
                start_date = time_range['start_date']
                end_date = time_range['end_date']
                if isinstance(start_date, str):
                    start_date = datetime.strptime(start_date, "%d/%m/%Y")
                if isinstance(end_date, str):
                    end_date = datetime.strptime(end_date, "%d/%m/%Y")
                time_range_filter = {
                    "start_date": start_date.strftime("%d/%m/%Y"),
                    "end_date": end_date.strftime("%d/%m/%Y")
                }
                print(f"‚è∞ Time range filter: {time_range_filter['start_date']} - {time_range_filter['end_date']}")
        
        print(f"[Detect] Subject: {detected_subject} (score={subject_score:.3f}) | Type: {detected_type} (score={type_score:.3f})")
        
        # X√°c th·ª±c metadata (ch·ªâ khi LLM helper available)
        if self.llm_helper and detected_subject and hasattr(self.llm_helper, 'SUBJECTS') and detected_subject not in self.llm_helper.SUBJECTS:
            print(f"M√£ m√¥n h·ªçc '{detected_subject}' kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra l·∫°i truy v·∫•n.")
            return []
        if self.llm_helper and detected_type and hasattr(self.llm_helper, 'TYPES') and detected_type not in self.llm_helper.TYPES:
            print(f"Lo·∫°i d·ªØ li·ªáu '{detected_type}' kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra l·∫°i truy v·∫•n.")
            return []
        
        # T·∫°o embedding cho query
        query_embedding = self.embedder.encode([self.prefix + query], normalize_embeddings=True)[0]
        
        # X√¢y d·ª±ng filters
        must = []
        should = []
        
        if detected_type:
            must.append(FieldCondition(key="loai", match=MatchValue(value=detected_type)))
        
        if detected_subject:
            should.append(FieldCondition(key="ma_mon_hoc", match=MatchValue(value=detected_subject)))
        
        # Th√™m time range filter n·∫øu c√≥
        if time_range_filter:
            # Qdrant can filter string dates with gte/lte if they're in sortable format
            # Our dates are in DD/MM/YYYY format which is sortable as strings
            must.append(FieldCondition(
                key="ngay",
                range=Range(
                    gte=time_range_filter["start_date"],
                    lte=time_range_filter["end_date"]
                )
            ))
            print(f"‚è∞ Time range filter applied in Qdrant: {time_range_filter['start_date']} - {time_range_filter['end_date']}")
        
        # T·∫°o Qdrant filter
        qdrant_filter = Filter(must=must, should=should) if must or should else None
        
        # Search trong Qdrant
        try:
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=qdrant_filter,
                limit=limit * 2,  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ LLM re-rank
                score_threshold=0
            )
            
            print(f"üìä Found {len(results)} results for user {user_id}")
            
            # Format k·∫øt qu·∫£
            formatted_results = []
            for i, hit in enumerate(results, 1):
                result = {
                    "rank": i,
                    "score": hit.score,
                    "loai": hit.payload.get("loai", "N/A"),
                    "ma_mon_hoc": hit.payload.get("ma_mon_hoc", "N/A"),
                    "ten_mon_hoc": hit.payload.get("ten_mon_hoc", "N/A"),
                    "content": hit.payload.get("noi_dung", "")[:300] + ("..." if len(hit.payload.get("noi_dung", "")) > 300 else "")
                }
                formatted_results.append(result)
            
            # LLM Re-rank (n·∫øu enabled)
            if self.enable_llm and self.llm_helper and formatted_results:
                original_results = formatted_results.copy()
                formatted_results = self.llm_helper.re_rank_results(query, formatted_results, limit)
                # C·∫≠p nh·∫≠t l·∫°i rank sau khi re-rank
                for i, result in enumerate(formatted_results):
                    result['rank'] = i + 1
            
            # In k·∫øt qu·∫£
            for i, result in enumerate(formatted_results[:limit], 1):
                print(f"  {i}. Score: {result['score']:.3f} | Loai: {result['loai']} | Mon hoc: {result['ma_mon_hoc']} - {result['ten_mon_hoc']}")
                print(f"     Content: {result['content']}")
            
            return formatted_results[:limit]
            
        except Exception as e:
            print(f"‚ùå Search error: {e}")
            return []

    def run_full_embedding_pipeline_from_db(self, user_id, df_profile, df_attendance, df_grades, df_courses):
        """
        Pipeline: chunk -> check hash -> embedding -> upsert Qdrant -> t·∫°o index detection
        Ch·ªâ embedding c√°c payload m·ªõi (hash ch∆∞a c√≥)
        """
        # L·∫•y t√™n user
        user_full_name = None
        if not df_profile.empty:
            user_full_name = str(df_profile.iloc[0].get('full_name', ''))
        all_payloads = []
        if not df_profile.empty:
            profile_payloads = self.chunk_student_profile(df_profile, user_full_name)
            all_payloads.extend(profile_payloads)
            print(f"üìã Student profile: {len(profile_payloads)} payloads")
        else:
            print("‚ö†Ô∏è  No student profile data")
            
        if not df_attendance.empty:
            attendance_payloads = self.chunk_attendance_reports(df_attendance, user_full_name)
            all_payloads.extend(attendance_payloads)
            print(f"üìã Attendance: {len(attendance_payloads)} payloads")
        else:
            print("‚ö†Ô∏è  No attendance data")
            
        if not df_grades.empty:
            grades_payloads = self.chunk_grade_details(df_grades, user_full_name)
            all_payloads.extend(grades_payloads)
            print(f"üìã Grades: {len(grades_payloads)} payloads")
        else:
            print("‚ö†Ô∏è  No grades data")
            
        if not df_courses.empty:
            courses_payloads = self.chunk_course_summaries(df_courses, user_full_name)
            all_payloads.extend(courses_payloads)
            print(f"üìã Course summaries: {len(courses_payloads)} payloads")
        else:
            print("‚ö†Ô∏è  No course summaries data")
            
        print(f"üìä Total payloads: {len(all_payloads)}")
        # Check hash
        existing_hashes = self.get_existing_hashes()
        new_payloads = [p for p in all_payloads if p['content_hash'] not in existing_hashes]
        print(f"New payloads to embed: {len(new_payloads)}")
        if not new_payloads:
            print("No new data to embed.")
            return 0
        # Embedding
        embeddings = self.generate_content_embedding(new_payloads)
        points = self.merge_point_structs(new_payloads, embeddings)
        self.safe_upsert_to_qdrant(points)
        self.create_payload_index()
        self.create_subject_embeddings()
        self.create_type_embeddings()
        self.create_term_embeddings()
        return len(new_payloads)

    def search_with_metadata(self, query: str, limit: int = 5):
        """
        H√†m search ƒë∆°n gi·∫£n s·ª≠ d·ª•ng metadata t·ª´ LLM
        """
        print(f"üîç Searching with metadata: '{query}'")
        
        # LLM Extract Intent
        llm_intent = {}
        if self.enable_llm and self.llm_helper:
            llm_intent = self.llm_helper.extract_query_intent(query)
            print(f"ü§ñ LLM Metadata: {llm_intent}")
        
        # T·∫°o embedding cho query
        query_embedding = self.embedder.encode([self.prefix + query], normalize_embeddings=True)[0]
        
        # X√¢y d·ª±ng filters t·ª´ metadata
        must = []
        should = []
        
        if llm_intent.get('loai'):
            must.append(FieldCondition(key="loai", match=MatchValue(value=llm_intent['loai'])))
            print(f"üîç Filter by loai: {llm_intent['loai']}")
        
        if llm_intent.get('ma_mon_hoc'):
            should.append(FieldCondition(key="ma_mon_hoc", match=MatchValue(value=llm_intent['ma_mon_hoc'])))
            print(f"üîç Filter by ma_mon_hoc: {llm_intent['ma_mon_hoc']}")
        
        # T·∫°o Qdrant filter
        qdrant_filter = Filter(must=must, should=should) if must or should else None
        
        # Search trong Qdrant
        try:
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=qdrant_filter,
                limit=limit,
                score_threshold=0
            )
            
            print(f"üìä Found {len(results)} results")
            
            # Format k·∫øt qu·∫£
            formatted_results = []
            for i, hit in enumerate(results, 1):
                result = {
                    "rank": i,
                    "score": hit.score,
                    "loai": hit.payload.get("loai", "N/A"),
                    "ma_mon_hoc": hit.payload.get("ma_mon_hoc", "N/A"),
                    "ten_mon_hoc": hit.payload.get("ten_mon_hoc", "N/A"),
                    "content": hit.payload.get("noi_dung", "")[:200] + ("..." if len(hit.payload.get("noi_dung", "")) > 200 else "")
                }
                formatted_results.append(result)
            
            # In k·∫øt qu·∫£
            for i, result in enumerate(formatted_results, 1):
                print(f"  {i}. Score: {result['score']:.3f}")
                print(f"     Loai: {result['loai']}")
                print(f"     Mon hoc: {result['ma_mon_hoc']} - {result['ten_mon_hoc']}")
                print(f"     Content: {result['content']}")
                print()
            
            return formatted_results
            
        except Exception as e:
            print(f"‚ùå Search error: {e}")
            return []

    def get_existing_hashes_for_user(self, user_id: str):
        """
        L·∫•y t·∫•t c·∫£ content_hash ƒë√£ c√≥ trong collection Qdrant cho m·ªôt user c·ª• th·ªÉ.
        """
        existing_hashes = set()
        try:
            scroll = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=Filter(
                    must=[FieldCondition(key="user_id", match=MatchValue(value=user_id))]
                ),
                limit=10000,
                with_payload=["content_hash"]
            )
            for point in scroll[0]:
                h = point.payload.get("content_hash")
                if h:
                    existing_hashes.add(h)
            print(f"üìä Found {len(existing_hashes)} existing hashes for user {user_id}")
        except Exception as e:
            print(f"‚ùå Error getting existing hashes for user {user_id}: {e}")
        return existing_hashes

    def check_duplicates_for_user(self, user_id: str):
        """
        Ki·ªÉm tra tr√πng l·∫∑p d·ªØ li·ªáu cho m·ªôt user c·ª• th·ªÉ.
        """
        try:
            # L·∫•y t·∫•t c·∫£ records c·ªßa user
            scroll = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=Filter(
                    must=[FieldCondition(key="user_id", match=MatchValue(value=user_id))]
                ),
                limit=10000,
                with_payload=True
            )
            
            # Ph√¢n t√≠ch tr√πng l·∫∑p
            content_hashes = {}
            duplicates = []
            
            for point in scroll[0]:
                content_hash = point.payload.get("content_hash")
                if content_hash:
                    if content_hash in content_hashes:
                        duplicates.append({
                            "hash": content_hash,
                            "existing_id": content_hashes[content_hash],
                            "duplicate_id": point.id,
                            "content": point.payload.get("noi_dung", "")[:100]
                        })
                    else:
                        content_hashes[content_hash] = point.id
            
            print(f"üîç Duplicate check for user {user_id}:")
            print(f"   Total records: {len(scroll[0])}")
            print(f"   Unique content hashes: {len(content_hashes)}")
            print(f"   Duplicates found: {len(duplicates)}")
            
            if duplicates:
                print("   Duplicate details:")
                for dup in duplicates[:5]:  # Ch·ªâ hi·ªÉn th·ªã 5 duplicate ƒë·∫ßu ti√™n
                    print(f"     - Hash: {dup['hash'][:20]}...")
                    print(f"       Content: {dup['content']}")
            
            return {
                "total_records": len(scroll[0]),
                "unique_hashes": len(content_hashes),
                "duplicates": duplicates
            }
            
        except Exception as e:
            print(f"‚ùå Error checking duplicates for user {user_id}: {e}")
            return None

# ===== USAGE EXAMPLE =====
def main():
    """Example usage"""
    # C·∫•u h√¨nh paths
    # Use relative paths instead of hardcoded paths
    data_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'FAP')
    csv_paths = {
        'student_profile': os.path.join(data_dir, 'student_profile.csv'),
        'attendance_reports': os.path.join(data_dir, 'attendance_reports.csv'), 
        'grade_details': os.path.join(data_dir, 'grade_details.csv'),
        'course_summaries': os.path.join(data_dir, 'course_summaries.csv')
    }
    # Qdrant config s·∫Ω l·∫•y t·ª´ .env n·∫øu kh√¥ng truy·ªÅn v√†o
    engine = FapSearchEngine(csv_paths=csv_paths)
    
    # K·∫øt n·ªëi th·ª≠ Qdrant
    # Load data v√† t·∫°o embeddings
    engine.load_all_dataframes()
    
    # T·∫°o c√°c payload
    all_payloads = []
    for df_name, df in engine.dataframes.items():
        if df_name == 'student_profile':
            all_payloads.extend(engine.chunk_student_profile(df))
        elif df_name == 'attendance_reports':
            all_payloads.extend(engine.chunk_attendance_reports(df))
        elif df_name == 'grade_details':
            all_payloads.extend(engine.chunk_grade_details(df))
        elif df_name == 'course_summaries':
            all_payloads.extend(engine.chunk_course_summaries(df))
    
    # T·∫°o embeddings v√† upload
    embeddings = engine.generate_content_embedding(all_payloads)
    points = engine.merge_point_structs(all_payloads, embeddings)
    engine.safe_upsert_to_qdrant(points)
    engine.create_payload_index()
    
    # # T·∫°o detection embeddings
    engine.create_subject_embeddings()
    engine.create_type_embeddings()
    engine.create_term_embeddings()
    
    # Test search
    results = engine.search_qdrant("ƒëi·ªÉm trung b√¨nh cpv", limit=5)
    print(results)
    print("\nüîë L∆∞u √Ω: ƒê·∫∑t file .env v·ªõi c√°c bi·∫øn QDRANT_URL, QDRANT_API_KEY, QDRANT_COLLECTION n·∫øu mu·ªën c·∫•u h√¨nh ri√™ng!")
    return engine

if __name__ == "__main__":
    main()